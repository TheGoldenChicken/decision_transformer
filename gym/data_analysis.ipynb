{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dict()\n",
    "\n",
    "envs = ['halfcheetah', 'hopper', 'walker2d']\n",
    "datasets = ['expert', 'medium_replay', 'medium']\n",
    "data_features = ['observations', 'next_observations', 'actions', 'rewards', 'terminals']\n",
    "\n",
    "for e in envs:\n",
    "    for d in datasets:\n",
    "        with open(f'./data_plus_plus/{e}-{d}-v2.pkl', 'rb') as f:\n",
    "            train_data[f'{e}-{d}'] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:18<00:00, 15.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# for each dataset, combine all trajectories into a single long one\n",
    "def concatenate_dataset(dataset):\n",
    "    keys = list(dataset[0].keys())\n",
    "\n",
    "    total_dict = dict()\n",
    "    for key in keys:\n",
    "        total_dict[key] = dataset[0][key]\n",
    "\n",
    "    for traj in dataset[1:]:\n",
    "        for key in keys:\n",
    "            total_dict[key] = np.append(total_dict[key], traj[key], axis=0)\n",
    "    \n",
    "    return total_dict\n",
    "\n",
    "concatenated_train_data = {k: concatenate_dataset(v) for k, v in tqdm(train_data.items())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_terminals</th>\n",
       "      <th>mean_terminal_reward</th>\n",
       "      <th>std_terminal_reward</th>\n",
       "      <th>n_traj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>halfcheetah-expert</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10656.426460</td>\n",
       "      <td>441.682728</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>halfcheetah-medium_replay</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3093.285581</td>\n",
       "      <td>1680.693937</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>halfcheetah-medium</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4770.334765</td>\n",
       "      <td>355.750394</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hopper-expert</th>\n",
       "      <td>0.086660</td>\n",
       "      <td>3511.357707</td>\n",
       "      <td>328.585955</td>\n",
       "      <td>1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hopper-medium_replay</th>\n",
       "      <td>0.803038</td>\n",
       "      <td>467.302044</td>\n",
       "      <td>511.025583</td>\n",
       "      <td>2041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hopper-medium</th>\n",
       "      <td>0.999543</td>\n",
       "      <td>1422.056180</td>\n",
       "      <td>378.953696</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walker2d-expert</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>4920.507113</td>\n",
       "      <td>136.394925</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walker2d-medium_replay</th>\n",
       "      <td>0.723696</td>\n",
       "      <td>682.701247</td>\n",
       "      <td>895.955582</td>\n",
       "      <td>1093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walker2d-medium</th>\n",
       "      <td>0.431092</td>\n",
       "      <td>2852.088416</td>\n",
       "      <td>1095.443313</td>\n",
       "      <td>1190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mean_terminals  mean_terminal_reward  \\\n",
       "halfcheetah-expert               0.000000          10656.426460   \n",
       "halfcheetah-medium_replay        0.000000           3093.285581   \n",
       "halfcheetah-medium               0.000000           4770.334765   \n",
       "hopper-expert                    0.086660           3511.357707   \n",
       "hopper-medium_replay             0.803038            467.302044   \n",
       "hopper-medium                    0.999543           1422.056180   \n",
       "walker2d-expert                  0.001000           4920.507113   \n",
       "walker2d-medium_replay           0.723696            682.701247   \n",
       "walker2d-medium                  0.431092           2852.088416   \n",
       "\n",
       "                           std_terminal_reward  n_traj  \n",
       "halfcheetah-expert                  441.682728    1000  \n",
       "halfcheetah-medium_replay          1680.693937     202  \n",
       "halfcheetah-medium                  355.750394    1000  \n",
       "hopper-expert                       328.585955    1027  \n",
       "hopper-medium_replay                511.025583    2041  \n",
       "hopper-medium                       378.953696    2186  \n",
       "walker2d-expert                     136.394925    1000  \n",
       "walker2d-medium_replay              895.955582    1093  \n",
       "walker2d-medium                    1095.443313    1190  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = [f'{e}-{d}' for e in envs for d in datasets]\n",
    "df_train_data = pd.DataFrame(index=idxs)\n",
    "\n",
    "df_train_data['mean_terminals'] = [np.mean([sum(data['terminals']) for data in train_data[idx]]) for idx in idxs]\n",
    "df_train_data['mean_terminal_reward'] = [np.mean([sum(data['rewards']) for data in train_data[idx]]) for idx in idxs]\n",
    "df_train_data['std_terminal_reward'] = [np.std([sum(data['rewards']) for data in train_data[idx]]) for idx in idxs]\n",
    "df_train_data['n_traj'] = [len(train_data[idx]) for idx in idxs]\n",
    "\n",
    "df_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def get_forward_reward(positions):\n",
    "    return positions[1:] - positions[:-1]\n",
    "\n",
    "\n",
    "def get_control_cost(actions):\n",
    "    return np.sum(np.square(actions), axis=1)\n",
    "\n",
    "\n",
    "# calculate the weights of the individual parts of the reward using linear regression\n",
    "def get_coef(data_temp, use_healthy_reward=False, use_intercept=False):\n",
    "\n",
    "    forward_reward = get_forward_reward(data_temp['infos/qpos'][:,0])\n",
    "    ctrl_cost = get_control_cost(data_temp['actions'])\n",
    "\n",
    "    target_reward = data_temp['rewards'][:-1]\n",
    "\n",
    "    if use_healthy_reward:\n",
    "        healthy_reward = np.array(~data_temp['terminals'], dtype=int)\n",
    "        X = np.stack((forward_reward, ctrl_cost[:-1], healthy_reward[:-1]), axis=0).T\n",
    "    else:\n",
    "        X = np.stack((forward_reward, ctrl_cost[:-1]), axis=0).T\n",
    "\n",
    "    reg = LinearRegression(fit_intercept=False).fit(X, target_reward)\n",
    "    \n",
    "    return reg, X, target_reward\n",
    "\n",
    "# r_score = reg.score(X, target_reward)\n",
    "# coef = reg.coef_ # forward_reward_weight, ctrl_cost_weight, (healthy_reward)\n",
    "# predictions = reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_r_scores, all_coefs, all_traj_lens = [], [], []\n",
    "\n",
    "for idx in idxs:\n",
    "    trajs = train_data[idx]\n",
    "\n",
    "    use_healthy_reward = idx.split('-')[0] == 'halfcheetah'\n",
    "\n",
    "    r_scores, coefs, total_traj_len = [], [], 0\n",
    "    for traj in trajs:\n",
    "        \n",
    "        if 7 < len(traj): # dunno why 7 works but less doesn't\n",
    "            reg, X, target_reward = get_coef(traj, use_healthy_reward=use_healthy_reward)\n",
    "\n",
    "            traj_len = len(traj)\n",
    "            total_traj_len += traj_len\n",
    "            r_scores.append(reg.score(X, target_reward))\n",
    "            coefs.append(reg.coef_)\n",
    "    \n",
    "    all_traj_lens.append(total_traj_len)\n",
    "    all_r_scores.append(r_scores)\n",
    "    all_coefs.append(coefs)\n",
    "\n",
    "df_train_data['r_scores'] = np.array(all_r_scores)\n",
    "df_train_data['coefs'] = np.array(all_coefs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half cheetah\n",
    "\n",
    "# choose dataset\n",
    "data_temp = train_data['halfcheetah-expert'][1]\n",
    "\n",
    "reg, X, target_reward = get_coef(data_temp, use_healthy_reward=True)\n",
    "\n",
    "print(reg.coef_, reg.score(X, target_reward))\n",
    "\n",
    "predictions = reg.predict(X)\n",
    "\n",
    "for i, X_1dim in enumerate(X.T):\n",
    "    \n",
    "    temp_X = X.copy()\n",
    "    temp_X[:,i] = 0\n",
    "\n",
    "    # plt.scatter(X_1dim, predictions - reg.predict(temp_X))\n",
    "    plt.scatter(X_1dim, predictions - target_reward, c='C0')\n",
    "    plt.hlines(0, min(X_1dim), max(X_1dim), linestyles='--', colors='C1')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(predictions, predictions - target_reward, c='C0')\n",
    "plt.hlines(0, min(predictions), max(predictions), linestyles='--', colors='C1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de to andre\n",
    "\n",
    "# choose dataset\n",
    "data_temp = train_data['walker2d-expert'][1]\n",
    "\n",
    "# get rewards\n",
    "forward_reward = get_forward_reward(data_temp['infos/qpos'][:,0])\n",
    "ctrl_cost = get_control_cost(data_temp['actions'])\n",
    "# healthy_reward = np.array(~data_temp['terminals'], dtype=int)\n",
    "target_reward = data_temp['rewards'][:-1]\n",
    "\n",
    "# define xs\n",
    "X = np.stack((forward_reward, ctrl_cost[:-1]), axis=0).T\n",
    "\n",
    "reg = LinearRegression().fit(X, target_reward)\n",
    "predictions = reg.predict(X)\n",
    "r_score = reg.score(X, target_reward)\n",
    "\n",
    "print(reg.coef_, r_score) # (forward_reward_weight, ctrl_cost_weight, healthy_reward), r_score\n",
    "\n",
    "plt.scatter(predictions, predictions - target_reward, c='C0')\n",
    "plt.hlines(0, min(predictions), max(predictions), linestyles='--', colors='C1')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# mask for points with low error\n",
    "mask = abs(predictions - target_reward ) < 0.002\n",
    "\n",
    "X_masked = np.stack((forward_reward, ctrl_cost[:-1]), axis=0).T[mask]\n",
    "target_reward_masked = data_temp['rewards'][:-1][mask]\n",
    "\n",
    "reg = LinearRegression().fit(X_masked, target_reward_masked)\n",
    "predictions = reg.predict(X_masked)\n",
    "r_score = reg.score(X_masked, target_reward_masked)\n",
    "\n",
    "print(reg.coef_, r_score) # (forward_reward_weight, ctrl_cost_weight, healthy_reward), r_score\n",
    "\n",
    "plt.scatter(predictions, predictions - target_reward_masked, c='C0')\n",
    "plt.hlines(0, min(predictions), max(predictions), linestyles='--', colors='C1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_reward_weights, ctrl_cost_weights, healthy_rewards, r_scores = [], [], [], []\n",
    "\n",
    "for idx in idxs:\n",
    "    e, d = idx.split('-')\n",
    "    total_dict = concatenated_train_data[idx]\n",
    "\n",
    "    if 1:\n",
    "        (forward_reward_weight, ctrl_cost_weight, healthy_reward), r_score = get_coef(total_dict)\n",
    "\n",
    "        forward_reward_weights.append(forward_reward_weight)\n",
    "        ctrl_cost_weights.append(ctrl_cost_weight)\n",
    "        healthy_rewards.append(healthy_reward)\n",
    "        r_scores.append(r_score)\n",
    "    else:\n",
    "        forward_reward_weights.append(0)\n",
    "        ctrl_cost_weights.append(0)\n",
    "        healthy_rewards.append(0)\n",
    "        r_scores.append(0)\n",
    "\n",
    "df_train_data['forward_reward_weight'] = forward_reward_weights\n",
    "df_train_data['ctrl_cost_weight'] = ctrl_cost_weights\n",
    "df_train_data['healthy_reward'] = healthy_rewards\n",
    "df_train_data['r_score'] = r_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp = concatenated_train_data['hopper-medium']\n",
    "\n",
    "\n",
    "forward_reward = data_temp['observations'][:,5]\n",
    "ctrl_cost = (data_temp['actions'] ** 2).sum(axis=1)\n",
    "healthy_reward = np.array(~data_temp['terminals'], dtype=int)\n",
    "\n",
    "target_reward = data_temp['rewards']\n",
    "\n",
    "\n",
    "X = np.stack((forward_reward[:-1], ctrl_cost[1:], healthy_reward[1:]), axis=0).T\n",
    "# X = forward_reward[:-1].reshape(-1, 1)\n",
    "y = target_reward[:-1]\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "r_score = reg.score(X, y)\n",
    "\n",
    "reg.coef_, r_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(12, 12)) # note that halfcheetah does not terminate early, has no \"survive\" feature\n",
    "fig.suptitle('Detribution of trajectory lenghts', size=30)\n",
    "\n",
    "for ax, (key, data) in zip(axs.flatten(), train_data.items()):\n",
    "    env, dataset = key.split('-')\n",
    "    lengths = [len(data['rewards']) for data in train_data[key]]\n",
    "    ax.set_title(f\"{env.capitalize()} {dataset}\")\n",
    "    n, bins, _ = ax.hist(lengths, bins=20)\n",
    "\n",
    "    median_val = np.median(lengths)\n",
    "    ax.vlines(median_val, 0, max(n), linestyles='--', colors='C3', label=f'Meadian ({int(median_val)})')\n",
    "    \n",
    "    mean_val = np.mean(lengths)\n",
    "    ax.vlines(mean_val, 0, max(n), linestyles='--', colors='C1', label=f'Mean ({int(mean_val)})')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "fig.suptitle('Destribution of trejectory total reward', size=30)\n",
    "\n",
    "for ax, (key, data) in zip(axs.flatten(), train_data.items()):\n",
    "    env, dataset = key.split('-')\n",
    "    terminal_rewards = [sum(data['rewards']) for data in train_data[key]]\n",
    "    ax.set_title(f\"{env.capitalize()} {dataset}\")\n",
    "    n, bins, _ = ax.hist(terminal_rewards, bins=20)\n",
    "\n",
    "    median_val = np.median(terminal_rewards)\n",
    "    ax.vlines(median_val, 0, max(n), linestyles='--', colors='C3', label=f'Meadian ({int(median_val)})')\n",
    "    \n",
    "    mean_val = np.mean(terminal_rewards)\n",
    "    ax.vlines(mean_val, 0, max(n), linestyles='--', colors='C1', label=f'Mean ({int(mean_val)})')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = './evaluation_data/'\n",
    "raw_eval_dict = dict()\n",
    "\n",
    "for env in os.listdir(eval_path):\n",
    "    eval_path = './evaluation_data/'\n",
    "\n",
    "    if not env == 'first_run_expert':\n",
    "\n",
    "        eval_path += env + '/'\n",
    "        for dataset in os.listdir(eval_path):\n",
    "            \n",
    "            eval_path += dataset + '/'\n",
    "            for filename in os.listdir(eval_path):\n",
    "                with open(eval_path + filename, 'rb') as f:\n",
    "                    raw_eval_dict[filename] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_to_DataFrama(raw_dict):\n",
    "\n",
    "    return_keys = [key for key in raw_dict.keys() if '_returns' in key]\n",
    "    temp_dict = dict()\n",
    "    for key in return_keys:\n",
    "        target_return, _ = key.split('_')\n",
    "        target_return = float(target_return)\n",
    "        temp_dict[target_return] = raw_dict[key]\n",
    "    \n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "eval_dict = {k: read_data_to_DataFrama(v) for k, v in raw_eval_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = '42'\n",
    "conf_int = (0.2, 0.8)\n",
    "\n",
    "\n",
    "def get_plot_statistics(df, conf_int=(0.05, 0.95)):\n",
    "\n",
    "    target_rewards = df.columns.to_numpy()\n",
    "    reward_mean = df.to_numpy().mean(axis=0)\n",
    "    reward_median = np.quantile(df.to_numpy(), 0.5, axis=0)\n",
    "    # reward_std = df.to_numpy().std(axis=0)\n",
    "    q_low, q_high = np.quantile(df.to_numpy(), conf_int, axis=0)\n",
    "\n",
    "    return target_rewards, reward_mean, reward_median, q_low, q_high\n",
    "\n",
    "\n",
    "plot_dict = {k: get_plot_statistics(v, conf_int=conf_int) for k, v in eval_dict.items() if k.split('-')[-1] == seed}\n",
    "\n",
    "side_len = int(np.ceil(np.sqrt(len(plot_dict))))\n",
    "fig, axs = plt.subplots(side_len, side_len, figsize=(side_len * 16 / 3, side_len * 16 / 3))\n",
    "\n",
    "for ax, (exp_name, values) in zip(axs.flatten(), plot_dict.items()):\n",
    "    x, y_mean, y_median, q_low, q_high = values\n",
    "\n",
    "    x_linear = np.linspace(min(x), max(x))\n",
    "    ax.plot(x_linear, x_linear, '--', c='C2', label='Oracle')\n",
    "    ax.plot(x, y_mean, c='C0', label='Mean DT')\n",
    "    ax.plot(x, y_median, c='C3', label='Median DT')\n",
    "    ax.fill_between(x, q_low, q_high, color='b', alpha=.1, label=f'conf int {int((conf_int[1]-conf_int[0])*100)}%')\n",
    "\n",
    "    train_data_name = '-'.join(np.take(exp_name.split('-'), (3,4)))\n",
    "    terminal_rewards = [sum(data['rewards']) for data in train_data[train_data_name]]\n",
    "    ax.vlines(max(terminal_rewards), min(min(y_mean), min(x_linear)),\n",
    "        max(max(y_mean), max(x_linear)), linestyles='--', colors='C1', label='Best Trajectory in Dataset')\n",
    "\n",
    "    iter_name, _, _, env_name, data_name, _ = exp_name.split('-')\n",
    "    ax.set_title(f\"{env_name.capitalize()} {data_name}, {iter_name[4:]} iterations\" )\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set_xlabel('Target reward')\n",
    "    ax.set_ylabel('Actual reward')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edf1248ce4e3c99c9d399bdf42a0872d380a744c04792fd925f76c0892383d91"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('decision-transformer-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
