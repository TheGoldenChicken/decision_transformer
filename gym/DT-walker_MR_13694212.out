W&B enabled, running your script from this directory will now sync to the cloud.
Disabling Weights & Biases. Run 'wandb login' again to re-enable.
Creating states list
==================================================
Starting new experiment: walker2d medium_replay
1093 trajectories, 300907 timesteps found
Average return: 680.20, std: 894.85
Max return: 4126.45, min: -48.73
==================================================
================================================================================
Iteration 1
time/training: 267.69997906684875
training/train_loss_mean: 0.20802410859167575
training/train_loss_std: 0.12144049959676288
training/action_error: 0.1429215520620346
================================================================================
Iteration 2
time/training: 307.24420166015625
training/train_loss_mean: 0.12311596370190382
training/train_loss_std: 0.012529797474625522
training/action_error: 0.11568979918956757
================================================================================
Iteration 3
time/training: 302.4124324321747
training/train_loss_mean: 0.1085892693862319
training/train_loss_std: 0.009981336356901752
training/action_error: 0.09792669117450714
================================================================================
Iteration 4
time/training: 304.50447130203247
training/train_loss_mean: 0.10095429646521807
training/train_loss_std: 0.009189635523375836
training/action_error: 0.11546481400728226
================================================================================
Iteration 5
time/training: 304.6056935787201
training/train_loss_mean: 0.09596388242468237
training/train_loss_std: 0.008487385036261117
training/action_error: 0.09387935698032379
================================================================================
Iteration 6
time/training: 266.80086064338684
training/train_loss_mean: 0.0923534382134676
training/train_loss_std: 0.007979704494250823
training/action_error: 0.0852513462305069
================================================================================
Iteration 7
time/training: 266.39609694480896
training/train_loss_mean: 0.08982556544691324
training/train_loss_std: 0.007629910205970986
training/action_error: 0.07972608506679535
================================================================================
Iteration 8
time/training: 302.2609488964081
training/train_loss_mean: 0.08733893816545606
training/train_loss_std: 0.007281817350972118
training/action_error: 0.08759313821792603
================================================================================
Iteration 9
time/training: 265.88267493247986
training/train_loss_mean: 0.0857359416525811
training/train_loss_std: 0.007148876733324456
training/action_error: 0.08143983036279678
================================================================================
Iteration 10
time/training: 305.2282409667969
training/train_loss_mean: 0.08384387312531472
training/train_loss_std: 0.006825127658477579
training/action_error: 0.09663072228431702
================================================================================
Iteration 11
time/training: 268.6474623680115
training/train_loss_mean: 0.08257187882885336
training/train_loss_std: 0.006744452561594078
training/action_error: 0.08042408525943756
================================================================================
Iteration 12
time/training: 301.34041380882263
training/train_loss_mean: 0.0813305156968534
training/train_loss_std: 0.006490742218870998
training/action_error: 0.07545822858810425

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 13694212: <DT-walker_MR> in cluster <dcc> Done

Job <DT-walker_MR> was submitted from host <n-62-30-7> by user <s204131> in cluster <dcc> at Fri Jun 10 10:26:02 2022
Job was executed on host(s) <4*n-62-20-6>, in queue <gpuv100>, as user <s204131> in cluster <dcc> at Fri Jun 10 10:26:02 2022
</zhome/94/3/155767> was used as the home directory.
</zhome/94/3/155767/decision_transformer/gym> was used as the working directory.
Started at Fri Jun 10 10:26:02 2022
Terminated at Fri Jun 10 13:27:40 2022
Results reported at Fri Jun 10 13:27:40 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
#BSUB -J DT-walker_MR
#BSUB -o DT-walker_MR_%J.out
#BSUB -e DT-walker_MR_%J.err
#BSUB -q gpuv100
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -n 4
#BSUB -R "rusage[mem=4G]"
#BSUB -R "span[hosts=1]"
#BSUB -W 24:00
# end of BSUB options


# load CUDA (for GPU support)

# activate the virtual environment
source $HOME/miniconda3/envs/decision-transformer-gym/bin/activate

wandb on
echo 'cad8b043f3731a2c453efd8f61915e186ac93ac3' | wandb login

python experiment_multiple_rewards.py --env 'walker2d' --dataset 'medium_replay' --save_iters '5,10,11,12' --eval_iters '8,9,10,11,12' --max_iters 12 --device 'cuda' --split_reward True --seed 43 --num_eval_episodes 20 --num_steps_per_iter 10000

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   10871.21 sec.
    Max Memory :                                 2657 MB
    Average Memory :                             2513.73 MB
    Total Requested Memory :                     16384.00 MB
    Delta Memory :                               13727.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   10981 sec.
    Turnaround time :                            10898 sec.

The output (if any) is above this job summary.



PS:

Read file <DT-walker_MR_13694212.err> for stderr output of this job.

