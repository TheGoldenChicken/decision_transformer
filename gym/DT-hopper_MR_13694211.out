W&B enabled, running your script from this directory will now sync to the cloud.
Disabling Weights & Biases. Run 'wandb login' again to re-enable.
Creating states list
==================================================
Starting new experiment: hopper medium_replay
2041 trajectories, 399959 timesteps found
Average return: 464.59, std: 510.32
Max return: 3189.35, min: -0.71
==================================================
================================================================================
Iteration 1
time/training: 242.03578996658325
training/train_loss_mean: 0.2497845499381423
training/train_loss_std: 0.09901533735433991
training/action_error: 0.17472928762435913
================================================================================
Iteration 2
time/training: 243.4931664466858
training/train_loss_mean: 0.16163697385787965
training/train_loss_std: 0.012463192809918032
training/action_error: 0.1558314561843872
================================================================================
Iteration 3
time/training: 243.7783465385437
training/train_loss_mean: 0.14229468128606676
training/train_loss_std: 0.009128878124722928
training/action_error: 0.1463560312986374
================================================================================
Iteration 4
time/training: 244.1899538040161
training/train_loss_mean: 0.13239979242309927
training/train_loss_std: 0.008096861259138097
training/action_error: 0.11761271208524704
================================================================================
Iteration 5
time/training: 244.35047841072083
training/train_loss_mean: 0.12565705248340964
training/train_loss_std: 0.007666882753358361
training/action_error: 0.11062609404325485
================================================================================
Iteration 6
time/training: 243.5187451839447
training/train_loss_mean: 0.12093968253955245
training/train_loss_std: 0.007273468863029139
training/action_error: 0.11985646188259125
================================================================================
Iteration 7
time/training: 242.9047555923462
training/train_loss_mean: 0.11744697097614408
training/train_loss_std: 0.007123931718640183
training/action_error: 0.12399110943078995
================================================================================
Iteration 8
time/training: 243.3024115562439
training/train_loss_mean: 0.1142894537307322
training/train_loss_std: 0.006793016476275162
training/action_error: 0.10581997781991959
================================================================================
Iteration 9
time/training: 243.3400936126709
training/train_loss_mean: 0.11203084026649594
training/train_loss_std: 0.006690001078217377
training/action_error: 0.11295635253190994
================================================================================
Iteration 10
time/training: 242.88849544525146
training/train_loss_mean: 0.10972909364625812
training/train_loss_std: 0.006425842909857234
training/action_error: 0.11133796721696854
================================================================================
Iteration 11
time/training: 243.4980320930481
training/train_loss_mean: 0.1078985116623342
training/train_loss_std: 0.006312781115748552
training/action_error: 0.10049121081829071
================================================================================
Iteration 12
time/training: 242.39730978012085
training/train_loss_mean: 0.10632485262900591
training/train_loss_std: 0.006129704148572488
training/action_error: 0.11110934615135193

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 13694211: <DT-hopper_MR> in cluster <dcc> Done

Job <DT-hopper_MR> was submitted from host <n-62-30-7> by user <s204131> in cluster <dcc> at Fri Jun 10 10:25:58 2022
Job was executed on host(s) <4*n-62-20-7>, in queue <gpuv100>, as user <s204131> in cluster <dcc> at Fri Jun 10 10:25:59 2022
</zhome/94/3/155767> was used as the home directory.
</zhome/94/3/155767/decision_transformer/gym> was used as the working directory.
Started at Fri Jun 10 10:25:59 2022
Terminated at Fri Jun 10 13:05:06 2022
Results reported at Fri Jun 10 13:05:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
#BSUB -J DT-hopper_MR
#BSUB -o DT-hopper_MR_%J.out
#BSUB -e DT-hopper_MR_%J.err
#BSUB -q gpuv100
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -n 4
#BSUB -R "rusage[mem=4G]"
#BSUB -R "span[hosts=1]"
#BSUB -W 24:00
# end of BSUB options


# load CUDA (for GPU support)

# activate the virtual environment
source $HOME/miniconda3/envs/decision-transformer-gym/bin/activate

wandb on
echo 'cad8b043f3731a2c453efd8f61915e186ac93ac3' | wandb login

python experiment_multiple_rewards.py --env 'hopper' --dataset 'medium_replay' --save_iters '5,10,11,12' --eval_iters '8,9,10,11,12' --max_iters 12 --device 'cuda' --split_reward True --seed 43 --num_eval_episodes 20 --num_steps_per_iter 10000

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   9517.74 sec.
    Max Memory :                                 2592 MB
    Average Memory :                             2498.82 MB
    Total Requested Memory :                     16384.00 MB
    Delta Memory :                               13792.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   9624 sec.
    Turnaround time :                            9548 sec.

The output (if any) is above this job summary.



PS:

Read file <DT-hopper_MR_13694211.err> for stderr output of this job.

